# import libraries and helper files
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
import os
import numpy as np
import pandas as pd
from flask import Flask, Blueprint, flash, g, redirect, render_template
from flask import request, session, url_for, jsonify
import openai
import inspect
from itertools import groupby
from subprocess import Popen, PIPE
from io import StringIO, BytesIO
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.figure import Figure
import base64
import sys
import re
from PIL import Image
import openai
from openai.embeddings_utils import get_embeddings, distances_from_embeddings
from openai.embeddings_utils import get_embedding, cosine_similarity
import pickle


# global declarations
global s1, s2, s3, s4, s5, s6, s7, s8, s9, s10, s11, s12, s13, s14, s15, s16
global cc_dict

'''
COMMAND-CODE DICTIONARY
'''
s1 = '''import pandas as pd
from sklearn.datasets import load_boston
boston = load_boston()
df_X = pd.DataFrame(boston.data)
df_X.columns = boston.feature_names
df_y = pd.DataFrame(boston.target)
df_y.columns = ['MEDV']
df = pd.concat([df_X, df_y], axis=1)
print(df.head())'''
s2 = '''print(df.describe())'''
s3 = '''print(df_X.columns)'''
s4 = '''print(df_y.columns)'''
s5 = '''corr = df['{0}'].corr(df['{1}'])
print('Correlation between {0} and {1}: ', corr)'''
s6 = '''import seaborn as sns
import numpy as np
mask = np.triu(np.ones_like(df.corr()))
sns.heatmap(df.corr(), xticklabels=df.columns, yticklabels=df.columns, mask=mask)'''
s7 = '''df.hist(column='{0}')'''
s8 = '''df.plot.scatter(x='{0}', y='{1}')'''
s9 = '''X = df.copy()
y = X.pop('{0}')'''
s10 = '''from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size={0})'''
s11 = '''from xgboost import XGBRegressor
model = XGBRegressor()
model.fit(X_train, y_train)
print('XGBRegressor parameters:')
print(model.get_xgb_params())'''
s12 = '''from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(X_train, y_train)
print('Random Forest Regressor parameters:')
print(model.get_params())'''
s13 = '''from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, y_train)
print('Logistic Regression parameters:')
print(model.get_params())'''
s14 = '''from sklearn.metrics import r2_score
y_pred = model.predict(X_{0})
model_r2_{0} = r2_score(y_{0}, y_pred)
print("{0} R2: ", model_r2_{0})'''
s15 = '''from sklearn.metrics import mean_absolute_error
y_pred = model.predict(X_{0})
model_mae_{0} = mean_absolute_error(y_{0}, y_pred)
print("{0} MAE: ", model_mae_{0})'''
s16 = ''''''
cc_dict = {'load data': s1,
           'summarize data': s2,
           'get feature names': s3,
           'get target name': s4,
           'get correlation between f1 and f2': s5,
           'show correlation heatmap': s6,
           'plot histogram of feature': s7,
           'show scatter plot of f1 and f2': s8,
           'set feature as target': s9,
           'train test split of given ratio': s10,
           'train an XGBoost model': s11,
           'train a random forest model': s12,
           'train a logistic regression model': s13,
           'calculate R2 score': s14,
           'calculate MAE score': s15}

'''
EMBEDDINGS
'''
cache_path = 'embeddings_cache.pkl'
try:
    embedding_cache = pd.read_pickle(cache_path)
    print('successfully loaded cached embeddings')
except FileNotFoundError:
    embedding_cache = get_embeddings(list(cc_dict.keys()),
                                     engine="text-similarity-davinci-001")
    with open(cache_path, "wb") as embedding_cache_file:
        pickle.dump(embedding_cache, embedding_cache_file)


'''
HELPER FUNCTIONS
'''
# store normal stdout in variable for reference
old_stdout = sys.stdout

# initialize dictionary for storing variables generated by code
ldict = {}


# helper function for running code stored in dictionary
def runcode(text, args=None):
    # turn off plotting and run function, try to grab fig and save in buffer
    plt.ioff()
    if args is None:
        exec(cc_dict[text], ldict)
    elif len(args) == 1:
        exec(cc_dict[text].format(args[0]), ldict)
    else:
        exec(cc_dict[text].format(*args), ldict)
    fig = plt.gcf()
    buf = BytesIO()
    fig.savefig(buf, format="png")
    plt.close()
    p = Image.open(buf)
    x = np.array(p.getdata(), dtype=np.uint8).reshape(p.size[1], p.size[0], -1)
    # if min and max colors are the same, it wasn't a plot - re-run as string
    if np.min(x) == np.max(x):
        new_stdout = StringIO()
        sys.stdout = new_stdout
        if args is None:
            exec(cc_dict[text], ldict)
        elif len(args) == 1:
            exec(cc_dict[text].format(args[0]), ldict)
        else:
            exec(cc_dict[text].format(*args), ldict)
        output = new_stdout.getvalue()
        sys.stdout = old_stdout
        outputtype = 'string'
        sys.stdout = old_stdout
        return [outputtype, output]
    # if it was a plot, then output as HTML image from buffer
    else:
        data = base64.b64encode(buf.getbuffer()).decode("ascii")
        output = f"<img src='data:image/png;base64,{data}'/>"
        outputtype = 'image'
        return [outputtype, output]


'''
FLASK APPLICATION CODE & ROUTES
'''
# set up flask application
app = Flask(__name__)


# base route to display main html body
@app.route('/', methods=["GET", "POST"])
def home():
    return render_template('icoder.html')


# run app in debug mode
if __name__ == "__main__":
    app.run(debug=True)


# create a function to read form inputs and process a set of outpts
# returns a json object containing:
    # 'txtcmd': text command
    # 'rawcode': stripped and formatted code text
    # 'output': console output of the code that was run
@app.route('/process')
def process():
    command = request.args.get('command')
    extra_args = []

    # check for any uppercase entries (feature names)
    feat_params = [a for a in command.split() if a.isupper()]
    if len(feat_params) > 0:
        extra_args.extend(feat_params)

    # turn to lowercase for uniformity
    command = command.lower()

    # parse command for any numbers
    num_params = re.findall(r'\s\d+', command)
    if len(num_params) > 0:
        nums = [float(n) for n in num_params]
        nums = [n/100 for n in nums if n > 1.0]
        nums = [str(round(n, 2)) for n in nums]
        extra_args.extend(nums)

    # check if train or test is mentioned
    if 'train' in command.lower():
        train_test = 'train'
        if len(num_params) == 0:
            extra_args.append(train_test)
    elif 'test' in command.lower():
        train_test = 'test'
        if len(num_params) == 0:
            extra_args.append(train_test)

    # check if command is in the dictionary keys; if not, match via embedding
    if command not in list(cc_dict.keys()):
        cmd_embed = get_embedding(command)
        sims = [cosine_similarity(cmd_embed, x) for x in embedding_cache]
        ind = np.argmax(sims)
        cmd = list(cc_dict.keys())[ind]
        code = list(cc_dict.values())[ind]
    else:
        cmd = command.lower()
        code = cc_dict[cmd]

    # supplement cmd with parameters (if applicable) and pass to runcode
    argtuple = tuple(extra_args)
    if len(argtuple) == 1:
        codeblock = code.format(argtuple[0])
    else:
        codeblock = code.format(*argtuple)
    print(codeblock)
    if len(argtuple) > 0:
        [outputtype, output] = runcode(cmd, argtuple)
    else:
        [outputtype, output] = runcode(cmd)
    outputs = [outputtype, command, codeblock, output]

    return jsonify(outputs=outputs)
